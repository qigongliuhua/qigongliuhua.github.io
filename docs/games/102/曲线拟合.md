# 1　多项式插值

## 1.1 问题描述

​	有已知数据 $(x_i, y_i)(i=0,1,\dots,n)$ 和曲线 $y(x)=a_0+a_1x+a_2x^2+\cdots+a_nx^n$ ，求 $y(x)$ 。

## 1.2 结论

​	有两种插值方法：拉格朗日插值和牛顿插值。

### 1.2.1 多项式插值

​	有 $n+1$ 个未知量（$a_i$），$n+1$ 个线性无关的方程组，故有唯一解。将数据点代入：
$$
\left\{\begin{array}{c}
y_0 = a_0 + a_1x_0 + a_2x_0^2 + \cdots + a_na_0^n \\
y_1 = a_0 + a_1x_1 + a_2x_1^2 + \cdots + a_na_1^n \\
\vdots \\
y_n = a_0 + a_1x_n + a_2x_n^2 + \cdots + a_na_n^n \\
\end{array}
\right.
$$
​	令：
$$
\mathbf{Y} = \begin{bmatrix}
y_0\\
y_1\\
\vdots\\
y_n
\end{bmatrix}, \qquad

\mathbf{B} = \begin{bmatrix}
1	&x_0	&x_0^2	&\cdots	&x_0^n \\
1	&x_1	&x_1^2	&\cdots	&x_1^n \\
\vdots	&\vdots	&\vdots	&\ddots	&\vdots\\
1	&x_n	&x_n^2	&\cdots	&x_n^n \\
\end{bmatrix}, \qquad 

\mathbf{\alpha} = \begin{bmatrix}
\alpha_0 \\
\alpha_1 \\
\vdots \\
\alpha_n
\end{bmatrix}
$$
​	则有线性方程组 $\mathbf{B}\mathbf{\alpha} = \mathbf{Y}$ ，求解即可得到 $\mathbf{\alpha}$ 。再带入曲线公式即可得到 $n$ 次多项式曲线：
$$
y(x)=a_0+a_1x+a_2x^2+\cdots+a_nx^n
$$

### 1.2.2 拉格朗日插值

​	$n$ 次插值基函数为
$$
l_i(x_k) = \frac{(x-x_0)\cdots(x-x_{k-1})(x-x_{k+1})\cdots(x-x_n)}
{(x_k-x_0)\cdots(x_k-x_{k-1})(x_k-x_{k+1})\cdots(x_k-x_n)},\quad k=0,1,\cdots,n
$$
​	则拉格朗日插值多项式为
$$
y(x) = \sum\limits_{k=0}^n y_kl_k(x_k)
$$

### 1.2.3 牛顿插值

​	均差定义如下：
$$
f[x_0,x_1,\cdots,x_k]=\frac{f[x_1,x_2,\cdots,x_k]-f[x_0,x_1,\cdots,x_{k-1}]}
{x_k-x_0} \tag{1}
$$
​	由**公式（1）**可推出下面这个*均差表*：

|  $x_k$   |       $f(x_k)$       |         一阶均差         |           二阶均差           |             三阶均差             |               四阶均差               |
| :------: | :------------------: | :----------------------: | :--------------------------: | :------------------------------: | :----------------------------------: |
|  $x_0$   | $\underline{f(x_0)}$ |                          |                              |                                  |                                      |
|  $x_1$   |       $f(x_1)$       | $\underline{f[x_0,x_1]}$ |                              |                                  |                                      |
|  $x_2$   |       $f(x_2)$       |       $f[x_1,x_2]$       | $\underline{f[x_0,x_1,x_2]}$ |                                  |                                      |
|  $x_3$   |       $f(x_3)$       |       $f[x_2,x_3]$       |       $f[x_1,x_2,x_3]$       | $\underline{f[x_0,x_1,x_2,x_3]}$ |                                      |
|  $x_4$   |       $f(x_4)$       |       $f[x_3,x_4]$       |       $f[x_2,x_3,x_4]$       |       $f[x_1,x_2,x_3,x_4]$       | $\underline{f[x_0,x_1,x_2,x_3,x_4]}$ |
| $\vdots$ |       $\vdots$       |         $\vdots$         |           $\vdots$           |             $\vdots$             |               $\vdots$               |

​	则可得到 $n$ 次多项式曲线：
$$
\begin{aligned}
y(x) = &f(x_0) +\\
& f[x_0,x_1](x-x_0)+ \\
&f[x_0,x_1,x_2](x-x_0)(x-x_1)+\\
&\cdots+\\
&f[x_0,x_1,\cdots,x_n](x-x_0)\cdots(x-x_{n-1})
\end{aligned}
$$

## 1.3 拉格朗日多项式插值公式推导（可忽略）

​	两种方法：数学归纳法和待定系数法。

### 1.3.1 数学归纳法

#### 1.3.1.1 一次的情况

​	已知线性方程组：
$$
\left\{\begin{array}{l}
y_0 = a_0+a_1x_0 \\
y_1 = a_0+a_1x_1
\end{array}\right. \qquad \text{解得：}\qquad
\left\{\begin{array}{l}
a_0 = \dfrac{y_1x_0-x_1y_0}{x_0-x_1} \\
a_1 = \dfrac{y_0-y_1}{x_0-x_1}
\end{array}\right.
$$
​	代入原式可得：
$$
y=\dfrac{y_1x_0-x_1y_0}{x_0-x_1} + \dfrac{y_0-y_1}{x_0-x_1} = \dfrac{x-x_1}{x_0-x_1}y_0+\dfrac{x-x_0}{x_1-x_0}y_1
$$

#### 1.3.1.2 二次的情况

​	已知线性方程组：
$$
\left\{\begin{array}{c}
y_0 = a_0 + a_1x_0 + a_2x_0^2 \\
y_1 = a_0 + a_1x_1 + a_2x_1^2 \\
y_2 = a_0 + a_1x_2 + a_2x_2^2 \\
\end{array}
\right.
$$
​	由克莱姆法则可知：
$$
a_0 = \frac{D_0}{|D|} = \frac{\begin{vmatrix}
y_0 &x_0 &x_0^2\\
y_1 &x_1 &x_1^2\\
y_2 &x_2 &x_2^2
\end{vmatrix}}{|D|} = \frac{
y_0\begin{vmatrix}x_1 &x_1^2 \\x_2 & x_2^2\end{vmatrix}-
y_1\begin{vmatrix}x_0 &x_0^2 \\x_2 & x_2^2\end{vmatrix}+
y_2\begin{vmatrix}x_0 &x_0^2 \\x_1 & x_1^2\end{vmatrix}
}{|D|} = \frac{y_0A_{11}+y_1A_{21}+y_2A_{31}}{|D|} \\

a_1 = \frac{D_1}{|D|} = \frac{\begin{vmatrix}
1 &y_0 &x_0^2\\
1 &y_1 &x_1^2\\
1 &y_2 &x_2^2
\end{vmatrix}}{|D|} = \frac{
-y_0\begin{vmatrix}1 &x_1^2 \\1 & x_2^2\end{vmatrix}+
y_1\begin{vmatrix}1 &x_0^2 \\1 & x_2^2\end{vmatrix}-
y_2\begin{vmatrix}1 &x_0^2 \\1 & x_1^2\end{vmatrix}
}{|D|} = \frac{y_0A_{12}+y_1A_{22}+y_2A_{32}}{|D|} \\

a_2 = \frac{D_2}{|D|} = \frac{\begin{vmatrix}
1 &x_0 &y_0\\
1 &x_1 &y_1\\
1 &x_2 &y_2
\end{vmatrix}}{|D|} = \frac{
y_0\begin{vmatrix}1 &x_1 \\1 & x_2\end{vmatrix}-
y_1\begin{vmatrix}1 &x_0 \\1 & x_2\end{vmatrix}+
y_2\begin{vmatrix}1 &x_0 \\1 & x_1\end{vmatrix}
}{|D|} = \frac{y_0A_{13}+y_1A_{23}+y_2A_{33}}{|D|}
$$
​	带入 $a_i$ 可知：
$$
\begin{aligned}
y=a_0+a_1x+a_2x^2 &= \frac{y_0A_{11}+y_1A_{21}+y_2A_{31}}{|D|}+\frac{y_0A_{12}+y_1A_{22}+y_2A_{32}}{|D|} + \frac{y_0A_{13}+y_1A_{23}+y_2A_{33}}{|D|}\\
&=\frac{A_{11}+A_{12}x+A_{13}x^2}{|D|}y_0+\frac{A_{21}+A_{22}x+A_{23}x^2}{|D|}y_1+\frac{A_{31}+A_{32}x+A_{33}x^2}{|D|}y_2 \\
\end{aligned}
$$
​	其中：
$$
\left.\begin{array}{r}
A_{11}+A_{12}x+A_{13}x^2 = \begin{vmatrix}
1 & x & x^2 \\
1 & x_1 & x_1^2 \\
1 & x_2 & x_2^2 
\end{vmatrix} = M_1 \\

A_{11}+A_{12}x+A_{13}x^2 = \begin{vmatrix}
1 & x_0 & x_0^2 \\
1 & x & x^2 \\
1 & x_2 & x_2^2 
\end{vmatrix} = M_2 \\

A_{31}+A_{32}x+A_{33}x^2 = \begin{vmatrix}
1 & x_0 & x_0^2 \\
1 & x_1 & x_1^2 \\
1 & x & x^2 
\end{vmatrix} = M_3 
\end{array}\right\}
\text{均 为 范 德 蒙 特 行 列 式}
$$
​	则有：
$$
\frac{M_1}{|D|} = \frac{(x-x_1)(x-x_2)\bcancel{(x_2-x_1)}}{(x_1-x_0)(x_2-x_0)\bcancel{(x_2-x_1)}}
=\frac{(x-x_1)(x-x_2)}{(x_0-x_1)(x_0-x_2)} \\
\frac{M_2}{|D|} = \frac{(x-x_0)(x-x_2)\bcancel{(x_2-x_0)}}{(x_1-x_0)(x_1-x_2)\bcancel{(x_2-x_0)}}
=\frac{(x-x_0)(x-x_2)}{(x_1-x_0)(x_1-x_2)} \\
\frac{M_1}{|D|} = \frac{(x-x_0)(x-x_1)\bcancel{(x_1-x_0)}}{(x_2-x_0)(x_2-x_1)\bcancel{(x_1-x_0)}}
=\frac{(x-x_0)(x-x_1)}{(x_2-x_0)(x_2-x_1)} \\
$$
​	代入原式可得：
$$
y(x)=\frac{(x-x_1)(x-x_2)}{(x_0-x_1)(x_0-x_2)}y_0 +\frac{(x-x_0)(x-x_2)}{(x_1-x_0)(x_1-x_2)}y_1+\frac{(x-x_0)(x-x_1)}{(x_2-x_0)(x_2-x_1)} y_2
$$

#### 1.3.1.3 n次的情况

​	对于一般情况有：
$$
\frac{M_i}{|D|} = \frac{(x-x_1)(x-x_2)\cdots(x-x_n)}
{(x_i-x_1)(x_i-x_2)\cdots(x_i-x_n)} = \prod\limits_{j=0,j\not=i}^n \frac{x-x_j}{x_i-x_j}
$$
​	代入可得：
$$
y(x) = \sum\limits_{i=0}^{n}\Big(y_i\prod\limits_{j=0,j\not=i}^n \frac{x-x_j}{x_i-x_j}  \Big)=\sum\limits_{i=0}^n y_il_i(x_i)
$$

### 1.3.2 待定系数法

​	以三次为例，已知有拉格朗日插值多项式：$y(x)=l_0(x)y_0+l_1(x)y_1+l_2(x)y_2$ ，其中：
$$
\left\{\begin{array}{l}
l_0(x_0) = 1, &l_1(x_0)=0, &l_2(x_0)=0\\
l_0(x_1) = 0, &l_1(x_1)=1, &l_2(x_1)=0\\
l_0(x_2) = 0, &l_1(x_2)=0, &l_2(x_2)=1
\end{array}\right.
$$
​	则可设 $l_0(x)=C(x-x_1)(x-x_2)$ ，又因 $l_0(x_0)=1$ ，则解得 $C=\dfrac{1}{(x_0-x_1)(x_0-x_2)}$ ，即 $l_0(x)=\dfrac{(x-x_1)(x-x_2)}{(x_0-x_1)(x_0-x_2)}$ 。同理可解得 $l_1(x)$ 和 $l_2(x)$ 。$l_i(x)$ 称为基函数。

## 1.4 结果图片参考



![image-20220613002326484](https://qglh-tuchuang.oss-cn-hangzhou.aliyuncs.com/markdown_img/202206130023554.png)



# 2 高斯基函数插值

## 2.1 问题描述

​	有已知数据 $(x_i, y_i)(i=0,1,\dots,n)$ 和函数 $y(x) = a+\displaystyle\sum\limits_{i=0}^n b_ig_i(x)$ ，其中 $g_i(x)=\exp\{-\dfrac{(x-x_i)^2}{2\sigma^2}\}$ ，求 $a$ 和 $b_i(i=0,1,\cdots,n)$ 

## 2.2 结论

​	共 n+2 个未知量。有 n+1 组线性无关的方程组，还需添加一个约束条件，这里选择所有未知量的和为1。
$$
\left\{\begin{array}{cc}
a+b_0g_0(x_0)+b_1g_1(x_0)+\cdots+b_ng_n(x_0)&=y_0 \\
a+b_0g_0(x_1)+b_1g_1(x_1)+\cdots+b_ng_n(x_1)&=y_1 \\
\vdots\\
a+b_0g_0(x_n)+b_1g_1(x_n)+\cdots+b_ng_n(x_n)&=y_n \\
a+b_0+b_1+\cdots+b_n &= 1
\end{array}\right.
$$
 令
$$
\mathbf{G}=\begin{bmatrix}
1 & g_0(x_0) & g_1(x_0) & \cdots & g_n(x_0) \\
1 & g_0(x_1) & g_1(x_1) & \cdots & g_n(x_1) \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & g_0(x_n) & g_1(x_n) & \cdots & g_n(x_n) \\
1 & 1 & 1 & \cdots & 1
\end{bmatrix}, \qquad

\mathbf{b}=\begin{bmatrix}
a \\ b_0 \\b_1 \\ \vdots \\ b_n
\end{bmatrix}, \qquad

\mathbf{Y}=\begin{bmatrix}
y_0\\ y_1 \\ \vdots \\ y_n \\1
\end{bmatrix}
$$
​	则解方程组 $\mathbf{Gb} = \mathbf{Y}$ 得 $\mathbf{b}$ ，带回函数  $y(x) = a+\displaystyle\sum\limits_{i=0}^n b_ig_i(x)$ 即可。

## 2.3 结果图片参考

![image-20220613002429885](https://qglh-tuchuang.oss-cn-hangzhou.aliyuncs.com/markdown_img/202206130024940.png)



# 3 最小二乘多项式拟合

## 3.1 幂基函数

​	固定幂基函数得最高项次数 $m(m<n)$ 。设拟合函数 $f(x)=\displaystyle\sum\limits_{i=0}^m \alpha_i B_i(x)$ ，其中 $B_i(x)=x^i$ 。将 $n+1$ 个数据点代入：
$$
\left\{\begin{array}{c}
\alpha_0 + \alpha_1x_0 + \alpha_2x_0^2+\cdots+\alpha_mx_0^m = y_0\\
\alpha_0 + \alpha_1x_1 + \alpha_2x_1^2+\cdots+\alpha_mx_1^m = y_1\\
\vdots \\
\alpha_0 + \alpha_1x_n + \alpha_2x_n^2+\cdots+\alpha_mx_n^m = y_n\\
\end{array}\right.
$$
​	方程组数高于未知量个数，故只有近似解，这里使用最小二乘方法求解。令 $G=\min\displaystyle\sum\limits_{i=0}^n\Big(y_i-\sum\limits_{j=0}^m \alpha_jx_i^j\Big)^2$ ，要求 $G$ 最小，则求偏导：
$$
\left\{\begin{array}{c}
\dfrac{\partial G}{\partial \alpha_0} = 
\displaystyle\sum\limits_{i=0}^n\Big[2(y_i-\sum\limits_{j=0}^m \alpha_j x_i^j)(-x_i^0) \Big] = 0\\

\dfrac{\partial G}{\partial \alpha_1} = 
\displaystyle\sum\limits_{i=0}^n\Big[2(y_i-\sum\limits_{j=0}^m \alpha_j x_i^j)(-x_i^1) \Big] = 0\\

\vdots \\

\dfrac{\partial G}{\partial \alpha_m} = 
\displaystyle\sum\limits_{i=0}^n\Big[2(y_i-\sum\limits_{j=0}^m \alpha_j x_i^j)(-x_i^m) \Big] = 0\\
\end{array}\right. \qquad

\Rightarrow \qquad 

\left\{\begin{array}{c}
\displaystyle\sum\limits_{i=0}^nx_i^0\displaystyle\sum\limits_{j=0}^m (x_i^j\alpha_j) = 
\displaystyle\sum\limits_{i=0}^n y_ix_i^0 \\
 
 \displaystyle\sum\limits_{i=0}^nx_i^1\displaystyle\sum\limits_{j=0}^m (x_i^j\alpha_j) = 
\displaystyle\sum\limits_{i=0}^n y_ix_i^1 \\

\vdots \\

\displaystyle\sum\limits_{i=0}^nx_i^m\displaystyle\sum\limits_{j=0}^m (x_i^j\alpha_j) = 
\displaystyle\sum\limits_{i=0}^n y_ix_i^m \\
 
\end{array}\right. \qquad
$$
​	即：
$$
\begin{bmatrix}
\sum\limits_{i=0}^n x_i^0 x_i^0 & \sum\limits_{i=0}^n x_i^0 x_i^1 & \cdots & \sum\limits_{i=0}^n x_i^0 x_i^m \\

\sum\limits_{i=0}^n x_i^1 x_i^0 & \sum\limits_{i=0}^n x_i^1 x_i^1 & \cdots & \sum\limits_{i=0}^n x_i^1 x_i^m \\

\vdots & \vdots & \ddots & \vdots \\

\sum\limits_{i=0}^n x_i^m x_i^0 & \sum\limits_{i=0}^n x_i^m x_i^1 & \cdots & \sum\limits_{i=0}^n x_i^m x_i^m 
\end{bmatrix}
\begin{bmatrix}
\alpha_0 \\ \alpha_1 \\ \vdots \\ \alpha_m
\end{bmatrix}
=
\begin{bmatrix}
\sum\limits_{i=0}^n y_ix_i^0 \\
\sum\limits_{i=0}^n y_ix_i^1 \\
\vdots \\
\sum\limits_{i=0}^n y_ix_i^m
\end{bmatrix}
$$
​	令：
$$
\mathbf{B}=\begin{bmatrix}
1 & x_0 & x_0^2 & \cdots & x_0^m \\
1 & x_1 & x_1^2 & \cdots & x_1^m \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_n & x_n^2 & \cdots & x_n^m
\end{bmatrix}_{(n+1)\times (m+1)} , \qquad

\mathbf{\Alpha}=\begin{bmatrix}
\alpha_0 \\ \alpha_1 \\ \vdots \\ \alpha_m
\end{bmatrix}_{m\times 1} , \qquad

\mathbf{Y}=\begin{bmatrix}
y_0 \\ y_1 \\ \vdots \\ y_n
\end{bmatrix}_{n\times 1}
$$
​	则有 $\mathbf{B^T B\Alpha} =\mathbf{B^T Y} $，解得 $\mathbf{A}$  代入 $f(x)=\displaystyle\sum\limits_{i=0}^m \alpha_i B_i(x)$ 即可。

## 3.2 一般情况

​	有几何函数  $f(x)=\displaystyle\sum\limits_{i=0}^m \alpha_i B_i(x)$ ，其中 $B_i(x)=\Phi_i(x)$ ，定义 $(f,g)_n = \sum\limits_{i=0}^nf(x_i)g(x_i)$ ，则根据最小二乘法有：
$$
\begin{bmatrix}
(\Phi_0, \Phi_0)_n & (\Phi_0, \Phi_1)_n & \cdots & (\Phi_0, \Phi_m)_n \\
(\Phi_1, \Phi_0)_n & (\Phi_1, \Phi_1)_n & \cdots & (\Phi_1, \Phi_m)_n \\
\vdots & \vdots & \ddots & \vdots \\
(\Phi_n, \Phi_0)_n & (\Phi_n, \Phi_1)_n & \cdots & (\Phi_n, \Phi_m)_n \\
\end{bmatrix}
\begin{bmatrix}
\alpha_0 \\ \alpha_1 \\ \vdots \\ \alpha_m
\end{bmatrix}
=
\begin{bmatrix}
(y, \Phi_0)_n \\
(y, \Phi_1)_n \\
\vdots \\
(y, \Phi_n)_n
\end{bmatrix}
$$
​	令：
$$
\mathbf{B} =\begin{bmatrix}
\Phi_0(x_0) & \Phi_1(x_0) & \cdots & \Phi_m(x_0) \\
\Phi_0(x_1) & \Phi_1(x_1) & \cdots & \Phi_m(x_1) \\
\vdots & \vdots & \ddots & \vdots \\
\Phi_0(x_n) & \Phi_1(x_n) & \cdots & \Phi_m(x_n)
\end{bmatrix} _{(n+1)\times (m+1)}, \qquad

\mathbf{\Alpha}=\begin{bmatrix}
\alpha_0 \\ \alpha_1 \\ \vdots \\ \alpha_m
\end{bmatrix}_{m\times 1} , \qquad

\mathbf{Y}=\begin{bmatrix}
y_0 \\ y_1 \\ \vdots \\ y_n
\end{bmatrix}_{n\times 1}
$$
​	则有 $\mathbf{B^T B\Alpha} =\mathbf{B^T Y} $，解得 $\mathbf{A}$  代入 $f(x)=\displaystyle\sum\limits_{i=0}^m \alpha_i B_i(x)$ 即可。

## 3.3 结果图片参考

![image-20220613002623602](https://qglh-tuchuang.oss-cn-hangzhou.aliyuncs.com/markdown_img/202206130026663.png)





# 4 最小二乘多项式拟合岭回归优化

​	令 $G = \min\Big[\sum\limits_{i=0}^n (y_i-\sum\limits_{j=0}^m \alpha_jx_i^j)^2 + \lambda\sum\limits_{i=0}^m \alpha_i^2 \Big]$ ，其中 $\lambda$ 为岭回归系数，对 $G$ 求偏导：
$$
\left\{\begin{array}{c}
\dfrac{\partial G}{\partial \alpha_0} = 
\displaystyle\sum\limits_{i=0}^n\Big[2(y_i-\sum\limits_{j=0}^m \alpha_j x_i^j)(-x_i^0) \Big] 
+2\lambda\alpha_0 = 0\\

\dfrac{\partial G}{\partial \alpha_1} = 
\displaystyle\sum\limits_{i=0}^n\Big[2(y_i-\sum\limits_{j=0}^m \alpha_j x_i^j)(-x_i^1) \Big]
+2\lambda\alpha_1 = 0\\

\vdots \\

\dfrac{\partial G}{\partial \alpha_m} = 
\displaystyle\sum\limits_{i=0}^n\Big[2(y_i-\sum\limits_{j=0}^m \alpha_j x_i^j)(-x_i^m) \Big]
+2\lambda\alpha_m = 0\\
\end{array}\right. \qquad

\Rightarrow \qquad 

\left\{\begin{array}{c}
\displaystyle\sum\limits_{i=0}^nx_i^0\displaystyle\sum\limits_{j=0}^m (x_i^j\alpha_j)+\lambda\alpha_0 = 
\displaystyle\sum\limits_{i=0}^n y_ix_i^0 \\
 
 \displaystyle\sum\limits_{i=0}^nx_i^1\displaystyle\sum\limits_{j=0}^m (x_i^j\alpha_j)+\lambda\alpha_1 = 
\displaystyle\sum\limits_{i=0}^n y_ix_i^1 \\

\vdots \\

\displaystyle\sum\limits_{i=0}^nx_i^m\displaystyle\sum\limits_{j=0}^m (x_i^j\alpha_j)+\lambda\alpha_m = 
\displaystyle\sum\limits_{i=0}^n y_ix_i^m \\
 
\end{array}\right. \qquad
$$


  即：
$$
\begin{bmatrix}
\sum\limits_{i=0}^n x_i^0 x_i^0+\lambda & \sum\limits_{i=0}^n x_i^0 x_i^1 & \cdots & \sum\limits_{i=0}^n x_i^0 x_i^m \\

\sum\limits_{i=0}^n x_i^1 x_i^0 & \sum\limits_{i=0}^n x_i^1 x_i^1+\lambda & \cdots & \sum\limits_{i=0}^n x_i^1 x_i^m \\

\vdots & \vdots & \ddots & \vdots \\

\sum\limits_{i=0}^n x_i^m x_i^0 & \sum\limits_{i=0}^n x_i^m x_i^1 & \cdots & \sum\limits_{i=0}^n x_i^m x_i^m +\lambda
\end{bmatrix}
\begin{bmatrix}
\alpha_0 \\ \alpha_1 \\ \vdots \\ \alpha_m
\end{bmatrix}
=
\begin{bmatrix}
\sum\limits_{i=0}^n y_ix_i^0 \\
\sum\limits_{i=0}^n y_ix_i^1 \\
\vdots \\
\sum\limits_{i=0}^n y_ix_i^m
\end{bmatrix}
$$
​	令：
$$
\mathbf{B}=\begin{bmatrix}
1 & x_0 & x_0^2 & \cdots & x_0^m \\
1 & x_1 & x_1^2 & \cdots & x_1^m \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_n & x_n^2 & \cdots & x_n^m
\end{bmatrix}_{(n+1)\times (m+1)} , \qquad

\mathbf{\Alpha}=\begin{bmatrix}
\alpha_0 \\ \alpha_1 \\ \vdots \\ \alpha_m
\end{bmatrix}_{m\times 1} , \qquad

\mathbf{Y}=\begin{bmatrix}
y_0 \\ y_1 \\ \vdots \\ y_n
\end{bmatrix}_{n\times 1}
$$
​	则有 $\mathbf{(B^T B+\lambda I)\Alpha} =\mathbf{B^T Y} $，解得 $\mathbf{A}$  代入 $f(x)=\displaystyle\sum\limits_{i=0}^m \alpha_i B_i(x)$ 即可。

​	图片参考：



![image-20220613002707362](https://qglh-tuchuang.oss-cn-hangzhou.aliyuncs.com/markdown_img/202206130027451.png)
