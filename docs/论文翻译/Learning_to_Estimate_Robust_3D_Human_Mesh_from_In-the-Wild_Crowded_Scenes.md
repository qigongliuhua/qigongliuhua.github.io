> Learning to Estimate Robust 3D Human Mesh from In-the-Wild Crowded Scenes



## Abstract

We consider the problem of recovering a single person’s 3D human mesh from in-the-wild crowded scenes. While much progress has been in 3D human mesh estimation, existing methods struggle when test input has crowded scenes. The first reason for the failure is a domain gap between training and testing data. A motion capture dataset, which provides accurate 3D labels for training, lacks crowd data and impedes a network from learning crowded scene-robust image features of a target person. The second reason is a feature processing that spatially averages the feature map of a localized bounding box containing multiple people. Averaging the whole feature map makes a target person’s feature indistinguishable from others. We present 3DCrowdNet that firstly explicitly targets in-the-wild crowded scenes and estimates a robust 3D human mesh by addressing the above issues. First, we leverage 2D human pose estimation that does not require a motion capture dataset with 3D labels for training and does not suffer from the domain gap. Second, we propose a joint-based regressor that distinguishes a target person’s feature from others. Our joint-based regressor preserves the spatial activation of a target by sampling features from the target’s joint locations and regresses human model parameters. As a result, 3DCrowdNet learns target-focused features and effectively excludes the irrelevant features of nearby persons. We conduct experiments on various benchmarks and prove the robustness of 3DCrowdNet to the in-the-wild crowded scenes both quantitatively and qualitatively. Codes are available here.

我们考虑从野外拥挤场景中恢复单个人的3D人体网格的问题。虽然在3D人体网格估计方面取得了很大进展，但当测试输入场景拥挤时，现有的方法会很困难。失败的第一个原因是训练和测试数据之间的领域差距。运动捕捉数据集为训练提供准确的3D标签，缺乏人群数据，妨碍网络学习目标人物的拥挤场景鲁棒图像特征。第二个原因是特征处理，它对包含多个人的局部边界框的特征图进行空间平均。平均整个特征图使目标人物的特征与其他人无法区分。我们提出了3DCrowdNet，它首先明确地以野外拥挤场景为目标，并通过解决上述问题来估计鲁棒的3D人体网格。首先，我们利用2D人体姿势估计，该估计不需要具有3D标签的运动捕捉数据集来进行训练，并且不会受到域间隙的影响。其次，我们提出了一种基于联合的回归模型，将目标人物的特征与其他人区分开来。我们的基于关节的回归器通过从目标的关节位置采样特征并回归人体模型参数来保持目标的空间激活。因此，3DCrowdNet学习目标聚焦特征，并有效地排除附近人的不相关特征。我们在各种基准上进行了实验，并从定量和定性两方面证明了3DCrowdNet对野外拥挤场景的鲁棒性。[此处提供代码](https://github.com/hongsukchoi/3DCrowdNet_RELEASE)。



![image-20221022155640664](https://qglh-tuchuang.oss-cn-hangzhou.aliyuncs.com/markdown_img/202210221556814.png)



## 1. Introduction

Extensive research has been committed to reconstructing an accurate 3D human mesh, which represent both the pose and shape of a human, from a single image. However, 3D human mesh estimation from in-the-wild crowded scenes has been barely studied, despite their common presence. Consequently, most of the previous works show results on scenes without inter-person occlusion and provide inaccurate results on crowded scenes. The inter-person occlusion is the essential challenge of in-the-wild crowded scenes, and many practical applications including abnormal behavior detection [8] and person re-identification [35] encounter such situations. This paper investigates the limitation of the current literature and proposes a novel method for robust 3D human mesh estimation from in-the-wild crowded scenes.

许多研究致力于从单个图像重建精确的表示人体的姿势和形状的3D人体网格。然而，尽管这些场景普遍存在，但从野外拥挤场景中进行3D人体网格估计的研究却很少。因此，以前的大多数作品都在没有人与人互相遮挡的场景中显示结果，并且在拥挤的场景中提供了不准确的结果。人与人之间的遮挡是野外拥挤场景中的基本挑战，许多实际应用，包括异常行为检测[8]和人的重新识别[35]都会遇到这种情况。本文研究了现有文献的局限性，并提出了一种新的方法，用于从野外拥挤场景中进行鲁棒的3D人体网格估计。

The currently dominant training strategy for human mesh recovery is mixed-batch training. It composes a minibatch with one-half data from a motion capture (MoCap) 3D dataset [13, 26] and the other from an in-the-wild 2D dataset [22]. To use the 2D dataset for supervision, 3D joints regressed from a predicted mesh are projected onto the image plane, and the distance with 2D annotations is computed. This way of mixing 3D and 2D data is well known to improve accuracy and generalization [17, 19]by implicitly inducing a neural network to benefit from accurate 3D annotations of the 3D data and diverse image appearances in the 2D data. The dominant approach of recent works [5, 9,19] is a model-based approach using a global feature vector, which obtains the feature vector with a deep convolutional neural network (CNN) and regresses the human model parameters (e.g. SMPL [24]) from it. First, they crop an image using a bounding box of a target person detected from off-the-shelf human detectors [10]. Then they process the target’s cropped image with a deep CNN and perform a global average pooling to obtain the global feature vector. The global feature vector is fed to a Multi-Layer Perceptron (MLP)-based regressor that regresses the mesh parameters. The 3D meshes are obtained by forwarding the parameters to the human model layers.

目前人类网格恢复的主要训练策略是混合批训练。它使用来自运动捕捉（MoCap）3D数据集[13,26]的一半数据和来自野外2D数据集[22]的另一半数据组成一个小批次。为了使用2D数据集进行监控，将从预测网格回归的3D关节投影到图像平面上，并计算带有2D注释的距离。众所周知，这种混合3D和2D数据的方式通过隐式诱导神经网络从3D数据的精确3D注释和2D中的不同图像外观中受益来提高准确性和泛化[17，19]。最近工作的主要方法[5,9,19]是使用全局特征向量的基于模型的方法，该方法使用深度卷积神经网络（CNN）获得特征向量，并从中回归人体模型参数（例如SMPL[24]）。首先，他们使用从现成的人体探测器检测到的目标人物的边界框裁剪图像[10]。然后，他们用深度CNN处理目标的裁剪图像，并执行全局平均池以获得全局特征向量。全局特征向量被馈送到基于多层感知器（MLP）的回归器，该回归器回归网格参数。通过将参数转发到人体模型层来获得3D网格。

While the recent works have shown reasonable results on standard benchmarks [13,46] based on the two wheels of the current literature, in-the-wild crowded scenes remain insurmountable due to the following two reasons. First, a large domain gap between training data from MoCap datasets and testing data from in-the-wild crowded scenes hinders a deep CNN from extracting proper image features of a target person. The domain gap arises from the presence of a human crowd, which entails diverse inter-person occlusion, interacting body poses, and indistinguishable cloth appearances (Figure 1a). The mixed-batch training alone is insufficient to overcome the domain gap, and existing methods struggle to acquire robust image features from in-the-wild crowded scenes, and produce inaccurate meshes (Figure 1b). Intuitively, this tells us that external guidance robust to the domain gap is required for a crowded scene-robust image feature, in addition to the mixed-batch training.

尽管最近的作品在基于当前文献的两个轮子的标准基准[13,46]上显示了合理的结果，但由于以下两个原因，在狂野拥挤的场景中仍然无法克服。首先，来自MoCap数据集的训练数据和来自野外拥挤场景的测试数据之间存在很大的领域差距，这阻碍了深度CNN提取目标人物的正确图像特征。领域差距源于人群的存在，这需要不同的人与人之间的遮挡、相互作用的身体姿势和无法区分的布料外观（图1a）。单独的混合批训练不足以克服领域差距，现有方法难以从拥挤的场景中获取鲁棒的图像特征，并产生不准确的网格（图1b）。直观地说，这告诉我们，除了混合批训练之外，对于拥挤场景的鲁棒图像特征，还需要对域间隙鲁棒的外部指导。

Next, the global average pooling on a deep CNN feature collapses the spatial information that distinguishes a target person’s feature from others. In-the-wild crowded scenes often involve overlapping people and inaccurate human bounding boxes. Thus, a bounding box of a target inevitably includes non-target people. A deep CNN feature retains features of these non-target people, and the global average pooling makes a target person’s feature indistinguishable from others. This confuses a regressor and makes it difficult to capture an accurate 3D pose of a target person. For instance, the regressor may miss human parts occluded by another person or predict a different person’s pose.

接下来，深度CNN特征的全球平均汇集会使区分目标人物特征与其他特征的空间信息崩溃。在野外拥挤的场景中，经常会出现重叠的人和不准确的人体边界框。因此，目标的边界框不可避免地包括非目标人。CNN的深度功能保留了这些非目标人群的特征，而全球平均汇集使目标人群的功能与其他人无法区分。这混淆了回归因子，使得难以捕捉目标人物的准确3D姿势。例如，回归因子可能会遗漏被另一个人遮挡的人体部位，或预测另一个人的姿势。

In this regard, we present 3DCrowdNet, a novel network that learns to estimate a single person’s robust 3D human mesh from in-the-wild crowded scenes. This study is one of the earliest works that explicitly tackle 3D human mesh estimation of a target person in a crowd. 3DCrowdNet addresses the two issues of previous works in two folds. First, we resolve the domain gap by explicitly guiding a deep CNN to extract a crowded scene-robust image feature using an off-the-shelf 2D pose estimator. Unlike methods targeting 3D geometry, the 2D pose estimator does not require depth supervision and is not trained on a MoCap dataset. Instead, it is trained only on in-the-wild datasets [21, 41] that have images containing human crowds and suffers less from a domain gap regarding the inference on crowded scenes. Consequently, the 2D pose estimator’s outputs provide strong evidence of a target person and help 3DCrowdNet pay attention to a target’s feature despite the challenges in in-the-wild crowded scenes.

在这方面，我们提出了3DCrowdNet，这是一个新颖的网络，它可以学习从拥挤的场景中估计单个人的健壮3D人体网格。这项研究是最早明确解决人群中目标人物的3D人体网格估计的工作之一。3DCrowdNet从两个方面解决了之前作品中的两个问题。首先，我们通过显式引导深度CNN使用现成的2D姿态估计器提取拥挤场景鲁棒图像特征来解决域间隙。与针对3D几何体的方法不同，2D姿态估计器不需要深度监控，也不需要在MoCap数据集上进行训练。相反，它只在野生数据集[21，41]中进行训练，这些数据集包含人类人群的图像，并且在拥挤场景中的推断方面较少受到领域差距的影响。因此，2D姿势估计器的输出提供了目标人物的有力证据，并帮助3DCrowdNet关注目标人物的特征，尽管在拥挤的场景中存在挑战。

Second, we propose a joint-based regressor that does not blow away the spatial activation of a target person in a feature map with the global average pooling. The joint-based regressor first predicts the spatial locations of joints. Then, it samples image features from a deep CNN feature map with the locations. In particular, we keep the sampling area small to exclude features of non-target people. The target person’s feature is distinguished from others, and human model parameters are regressed from the sampled image features. The joint-based regressor differs from the previous regressors that evenly aggregate people’s features regardless of the target. Figure 2depicts the overview of 3DCrowdNet.

第二，我们提出了一种基于联合的回归模型，该模型不会在具有全局平均池的特征图中破坏目标人物的空间激活。基于关节的回归器首先预测关节的空间位置。然后，它从带有位置的深度CNN特征图中提取图像特征。特别是，我们保持采样区域较小，以排除非目标人群的特征。将目标人物的特征与其他人区分开来，并从采样的图像特征回归人体模型参数。基于关节的回归器不同于先前的回归器，前者平均聚集了人的特征，而不管目标是什么。图2显示了3DCrowdNet的概述。

Note that 3DCrowdNet substantially differs from prior works [6, 25] that directly lift 2D estimation outputs to 3D(a) we focus on producing and leveraging image features of a target person in human crowds, and (b) such image features help 3DCrowdNet to resolve the depth and shape ambiguity of a target person, from which the 2D estimation outputs inherently suffer. Thus, we argue that this work takes a step towards accurate 3D human mesh estimation from in-the-wild crowded scenes by distinguishing image features of a target person in densely interacting crowds, which is highly challenging but important. The experiments show that 3DCrowdNet significantly outperforms the previous 3D human mesh estimation methods on in-the-wild crowded scenes. Also, it achieves state-of-the-art accuracy in multiple 3D benchmarks [16, 28, 46]. Extensive qualitative results are presented in the main manuscript and supplementary material. Our contributions can be summarized as follows:

请注意，3DCrowdNet与直接将2D估计输出提升为3D的现有工作[6，25]有很大不同。因此，我们认为，这项工作通过区分密集交互人群中目标人物的图像特征，朝着从野生拥挤场景中准确估计3D人体网格迈出了一步，这是极具挑战性但很重要的。实验表明，在野生拥挤场景中，3DCrowdNet显著优于先前的3D人类网格估计方法。此外，它在多个3D基准中实现了最先进的精度[16，28，46]。主要手稿和补充材料中提供了大量的定性结果。我们的贡献可以总结如下：

+ We present 3DCrowdNet, the first approach to 3D human mesh recovery from in-the-wild crowded scenes. It effectively processes image features of a target person in a crowd, which is essential for accurate 3D pose and shape reconstruction.
+ 我们介绍了3DCrowdNet，这是第一种从野生拥挤场景中恢复3D人类网格的方法。它有效地处理人群中目标人物的图像特征，这对于准确的3D姿势和形状重建至关重要。
+ It extracts crowded scene-robust image features by resolving the domain gap with a 2D pose estimator.
+ 该算法通过二维姿态估计器求解域间隙来提取拥挤场景的鲁棒图像特征。
+ It distinguishes a target person’s image features from others using a joint-based regressor.
+ 它使用基于关节的回归量将目标人物的图像特征与其他人区分开来。
+ 3DCrowdNet significantly outperforms previous methods on in-the-wild crowded scenes both quantitatively and qualitatively, and achieves state-of-the-art 3D pose and shape accuracy on multiple 3D benchmarks.
+ 3DCrowdNet在数量和质量上都明显优于之前的方法，在多个3D基准上实现了最先进的3D姿势和形状精度。



## 2. Related works

**2D human pose estimation from crowded scenes**. Early works of 2D human pose estimation did not explicitly target crowded scenes. However, their methods are related to diverse challenges of in-the-wild crowded scenes, such as overlapping human bounding boxes, human detection error, and inter-person occlusion. There are two major approaches, namely bottom-up and top-down approaches. Bottom-up methods [2, 36, 40] first detect all joints of the people, and group them to each person. Top-down methods [3,10,37] first detect all human bounding boxes, and apply a single-person 2D pose estimation method to each person. Top-down methods generally achieve higher accuracy on traditional 2D pose benchmarks such as MSCOCO [22], but underperform on crowded scene benchmarks [21, 52] than bottom-up methods due to the human detection issues.

**拥挤场景中的2D人体姿势估计**。2D人体姿势估计的早期工作并未明确针对拥挤场景。然而，他们的方法与野生拥挤场景中的各种挑战相关，例如重叠的人体边界框、人体检测错误和人间遮挡。有两种主要方法，即自下而上和自上而下方法。自下而上的方法[2,36,40]首先检测人的所有关节，并将其分组给每个人。自顶向下的方法[3,10,37]首先检测所有人体边界框，并对每个人应用单个人2D姿势估计方法。自顶向下方法通常在传统的2D姿势基准（如MSCOCO[22]）上实现更高的精度，但由于人体检测问题，在拥挤场景基准[21，52]上表现不如自下而上方法。

Recently, a few works explicitly addressed crowded scenes 2D pose estimation and reported good accuracy on crowded scene benchmarks. [21] combined top-down and bottom-up approaches using joint-candidate single person pose estimation and global maximum joints association. [4] proposed to learn scale-aware representations using highresolution feature pyramids. [15] made a grouping process of the bottom-up approach differentiable using a graph neural network. [41] refined invisible joints’ prediction using an image-guided progressive graph convolutional network. 3D human geometry estimation from crowded scenes. Several methods [29, 47, 53] have shown reasonable results on multi-person 3D benchmarks [16, 26]. However, their focus was on absolute depth estimation of each person, and few works have addressed the inter-person occlusion to estimate robust 3D geometry, such as 3D human pose (i.e. 3D joint coordinates) and meshes, from in-the-wild crowded scenes. XNect [27] proposed an occlusion-robust method that can be applied to crowded scenes. However, it did not focus on resolving the domain gap. It integrated 2D/3D branches into a single system and trained it on a MoCap dataset [28], which barely contains inter-person occlusions. Also, it requires a particular joint (i.e. neck) must be visible for human detection. On the contrary, our key idea is leveraging external 2D pose estimators that are not trained on MoCap data, to alleviate the domain gap between MoCap training data and in-the-wild crowd testing data. In addition, 3DCrowdNet reconstructs full 3D human pose and shape from diverse partially invisible people in crowded scenes.

最近，一些工作明确地解决了拥挤场景的2D姿态估计，并报告了拥挤场景基准的良好精度。[21]使用联合候选单人姿势估计和全局最大关节关联的自上而下和自下而上组合方法。[4] 建议使用高分辨率特征金字塔学习尺度感知表示。[15] 使用图神经网络使自下而上方法的分组过程可微分。[41]使用图像引导渐进图卷积网络对不可见关节的预测进行细化。拥挤场景中的3D人体几何估计。几种方法[29，47，53]在多人3D基准[16，26]上显示了合理的结果。然而，他们的重点是对每个人的绝对深度估计，很少有工作涉及人与人之间的遮挡，以从拥挤的场景中估计鲁棒的3D几何体，例如3D人体姿势（即3D关节坐标）和网格。XNect[27]提出了一种可应用于拥挤场景的遮挡鲁棒方法。然而，它没有把重点放在解决领域差距上。它将2D/3D分支集成到单个系统中，并在MoCap数据集上对其进行训练[28]，该数据集几乎不包含人间闭塞。此外，它要求特定的关节（即颈部）必须可见，以便人类检测。相反，我们的关键思想是利用未经MoCap数据训练的外部2D姿态估计器，以缓解MoCap训练数据和野外人群测试数据之间的领域差距。此外，3DCrowdNet从拥挤场景中的各种部分不可见的人中重建了完整的3D人体姿势和形状。

ROMP [45] introduced a bottom-up method for multiperson 3D mesh recovery that can be applied to crowded scenes. It estimates a body center heatmap and a mesh parameter map, and samples each person’s mesh parameters from the parameter map using center locations regressed from the heatmap. While the method provides better results on crowded scenes than previous methods, it could still suffer from the domain gap between MoCap training data and testing data from in-the-wild crowded scenes. Also, solely relying on the body center estimation to distinguish a target from others could be unstable in cases of occlusion on the body center. On the other hand, 3DCrowdNet explicitly tackles the domain gap issue with crowded-scene robust 2D poses. Also, we utilize cues from multiple 2D joint locations of the target and refine image features sampled from the locations to handle diverse inter-person occlusion, including occlusion on the body center.

ROMP[45]引入了一种自底向上的多人3D网格恢复方法，可应用于拥挤场景。它估计身体中心热图和网格参数图，并使用从热图回归的中心位置从参数图中对每个人的网格参数进行采样。尽管该方法在拥挤场景中比以前的方法提供了更好的结果，但它仍然可能会受到MoCap训练数据和来自野生拥挤场景的测试数据之间的领域差距的影响。此外，在身体中心闭塞的情况下，仅依靠身体中心估计来区分目标与其他目标可能是不稳定的。另一方面，3DCrowdNet通过密集的场景鲁棒2D姿势明确解决了领域差距问题。此外，我们利用来自目标的多个2D关节位置的线索，并细化从这些位置采样的图像特征，以处理不同的人间遮挡，包括身体中心的遮挡。

**2D geometry to 3D human mesh estimation**. [6, 42, 43, 51] proposed methods that only take 2D geometry without images, such as 2D joint locations, for SMPL parameter regression. While the methods can benefit from 2D estimators robust to in-the-wild crowded scenes, they have two limitations. First, they cannot correct inaccurate 2D input compared to the actual person in images. Instead, they produce the most plausible outputs for the given 2D input, not the 3D pose and shape that best describes the person in images. Second, they do not benefit from image features with rich depth and 3D shape cues of a target person. The cues include subtle light reflection and shadows. 2D geometry hardly contains such cues and could lead to inaccurate 3D human mesh estimation. On the contrary, 3DCrowdNet reconstructs accurate 3D human meshes from possibly inaccurate 2D poses utilizing image features. Also, we focus on extracting the crowded scene-robust image feature of a target person using the 2D pose, rather than directly lifting 2D to 3D as the prior works.

**2D几何到3D人体网格估计**。[6，42，43，51]针对SMPL参数回归，提出了仅采用2D几何而不使用图像的方法，例如2D关节位置。尽管这些方法可以从2D估计器中受益，但它们在野外拥挤场景中具有鲁棒性，但它们有两个局限性。首先，与图像中的真实人相比，他们无法纠正不准确的2D输入。相反，它们为给定的2D输入生成最合理的输出，而不是最能描述图像中人物的3D姿势和形状。第二，他们无法从目标人物具有丰富深度和3D形状线索的图像特征中获益。这些线索包括细微的光线反射和阴影。2D几何几乎不包含这样的提示，并且可能导致不准确的3D人类网格估计。相反，3DCrowdNet利用图像特征从可能不准确的2D姿势重建准确的3D人体网格。此外，我们专注于使用2D姿势提取目标人物的拥挤场景鲁棒图像特征，而不是像先前的工作那样直接将2D提升到3D。



## 3. 3DCrowdNet

![image-20221022161713236](https://qglh-tuchuang.oss-cn-hangzhou.aliyuncs.com/markdown_img/202210221617283.png)



### 3.1. 3DCrowdNet architecture

As shown in Figure 2, our architecture comprises a feature extractor followed by a joint-based regressor. The feature extractor is based on ResNet-50 [11], and the jointbased regressor is based on [23, 33]. Our network’s output is SMPL [24] parameters, and a single person’s 3D mesh is obtained by feeding the parameters to the SMPL layer.

如图2所示，我们的体系结构包括一个特征提取器和一个基于关节的回归器。特征提取器基于ResNet-50[11]，基于连接的回归器基于[23，33]。我们的网络输出是SMPL[24]参数，通过将参数馈送到SMPL层，可以获得单个人的3D网格。

**Feature extractor**. The feature extractor takes a 2D pose and an image as input. The 2D pose is 2D joint coordinates $\mathbf{P^{2D}}\in \Reals^{J\times 2}$ predicted by bottom-up off-the-shelf 2D pose estimators [2, 4]. J denotes the number of human joints, and it can vary among different 2D pose estimators. During training, we add realistic errors on the ground truth (GT) 2D pose following [6,30] to mimic erroneous 2D pose outputs in test time, and the noisy 2D pose is used as our input $\mathbf{P^{2D}}$ . We provide the 2D pose $\mathbf{P^{2D}}$ as a heatmap representation $\mathbf{H^{2D}\in \Reals^{J_s \times 64 \times 64}}$ to the feature extractor by making a Gaussian blob on the 2D joint coordinates. $J_s=30$ indicates the number of joints in a superset of joint sets defined by multiple datasets. We assign don’t-care values to the undefined joints and joint predictions with low confidence in inference time, by multiplying zero to the corresponding joint’s heatmap. Modeling don’t-care values based on the superset of joints and heatmaps enables 3DCrowdNet to perform inference from various human joint sets with a single network and handle diverse input such as 2D poses with missing joints due to truncation and occlusion.

**功能提取器**。特征提取器将2D姿势和图像作为输入。2D姿势为2D关节坐标 $\mathbf{P^{2D}}\in \Reals^{J\times 2}$ 由自下而上的现成2D姿态估计器预测[2，4]。J表示人体关节的数量，它可以在不同的2D姿势估计器之间变化。在训练过程中，我们在地面实况（GT）2D姿态上添加真实误差[6,30]，以在测试时间内模拟错误的2D姿态输出，并且噪声2D姿态被用作我们的输入 $\mathbf{P^{2D}}$ 。我们提供2D姿势 $\mathbf{P^{2D}}$ 作为热图表示 $\mathbf{H^{2D}\in \Reals^{J_s \times 64 \times 64}}$ ，并发送给特征提取器，来在2D关节坐标上制作高斯斑点。$J_s=30$  表示由多个数据集定义的关节集的超集中的关节数。我们通过将0乘以相应关节的热图，为未定义的关节和关节预测指定非核心值，推断时间置信度较低。基于关节和热图的超集建模不重要的值使3DCrowdNet能够使用单个网络从各种人类关节集进行推断，并处理各种输入，例如由于截断和遮挡而丢失关节的2D姿势。

The feature extractor uses the 2D pose heatmap $\mathbf{H^{2D}}$ of a target person as guidance and pays attention to the spatial region of a target in a crowd. First, it obtains an early-stage image feature of ResNet $\mathbf{F}\in\Reals^{C\times64\times64}$ from a cropped image $\mathbf{I}\in \Reals^{3\times256\times256}$ . $C=64$ is the channel dimension, and $\mathbf{I}$ is acquired by cropping and resizing a bounding box area, derived from the 2D pose $\mathbf{P^{2D}}$ . Second, it concatenates $\mathbf{F}$ and $\mathbf{H^{2D}}$ along the channel dimension. The concatenated feature is processed by a 3-by-3 convolution block, which keeps the feature’s height and width but changes the channel dimension to $C$ . Finally, the feature with $C$ channels is fed back to the remaining part of ResNet, where the output is a crowded scene-robust image feature $\mathbf{F^\prime}\in \Reals^{C^\prime \times 8 \times 8}$ . $C^\prime = 2048$ is the channel dimension.

特征提取器使用目标人物的2D姿势热图 $\mathbf{H^{2D}}$ 作为指导，并关注人群中目标的空间区域。首先，它从裁剪后的图像 $\mathbf{I}\in\Reals^{3\times256\times256}$ 中获得残差神经网络（ResNet）$\mathbf{F}\in\Reals^{C\times64\times64}$ 的早期图像特征。$C=64$ 是通道维度，并且 $\mathbf{I}$ 通过裁剪和调整边界框区域的大小来获取，该边界框区域从2D姿势$\mathbf{P^{2D}}$ 获取。级联的特征由3乘3卷积块处理，该卷积块保持特征的高度和宽度，但将通道尺寸更改为 $C$ 。最后，具有 $C$ 通道的特征被反馈到ResNet的其余部分，其中输出是拥挤的场景鲁棒的图像特征 $\mathbf{F^\prime}\in \Reals^{C^\prime \times 8 \times 8}$。$C^\prime＝2048$ 是通道维度。

**Joint-based regressor**. The joint-based regressor first recovers 3D joint coordinates $\mathbf{P^{3D}} \in \Reals^{J_c\times 3}$ from $\mathbf{F}^\prime$ . $J_c=15$ denotes the number of joints in the intersection of joint sets defined by multiple datasets. $(x,y)$ values of $\mathbf{P^{3D}}$ are defined in a 2D pixel space, and $z$ value of $\mathbf{P^{3D}}$ represents root joint-relative depth. A 1-by-1 convolutional layer outputs a 3D heatmap $\mathbf{H^{3D}}\in \Reals^{J_c \times D \times 8\times 8}$ from $\mathbf{F}^\prime$ after predicting a $J_cD$ dimensional 2D feature map and reshaping it to the 3D heatmap. $D=8$ decides a descritizated size of depth. $\mathbf{P}^{3D}$ is computed from $\mathbf{H}^{3D}$ , using soft-argmax operation [44]. As the soft-argmax computes continuous coordinates from a discretized grid, we observed that a heatmap with a low resolution like $\mathbf{H}^{3D}$ gives similar accuracy compared to upsampled ones, while requiring less computational costs.

**基于关节的回归**。基于关节的回归器首先从 $\mathbf{F}^\prime$ 中恢复三维关节坐标 $\mathbf{P^{3D}} \in \Reals^{J_c\times 3}$ 。$J_c=15$ 表示由多个数据集定义的关节集的交集中的关节数。$\mathbf{P^{3D}}$ 的 $(x,y)$ 值在2D像素空间中定义，$\mathbf{P^{3D}}$ 的 $z$ 值表示根关节相对深度。1乘1卷积层在预测 $J_cD$ 维2D特征图并将其重塑为3D热图之后，从 $\mathbf{F}^\prime$ 输出3D热图$\mathbf{H^{3D}}\in \Reals^{J_c \times D \times 8\times 8}$ 。 $D=8$ 决定深度的描述大小。 $\mathbf{P}^{3D}$ 使用soft-argmax运算从 $\mathbf{H}^{3D}$ 计算而来[44]。当soft-argmax从离散化网格计算连续坐标时，我们观察到，像 $\mathbf{H}^{3D}$ 这样的低分辨率热图与上采样热图相比具有相似的精度，同时需要较少的计算成本。

Next, the joint-based regressor estimates global rotation of a person $\theta^g \in \Reals^3$ , SMPL body rotation parameters $\theta \in \Reals^{21\times3}$ , SMPL shape parameters $\beta \in \Reals^{10}$ , and camera parameters $k\in \Reals^3$ for projection. First, image features per joint are sampled from $\mathbf{F^\prime}$ using the $(x,y)$ pixel positions of $\mathbf{P^{3D}}$ . We use bilinear interpolation, since the $(x,y)$ pixel positions are not in discretized values. The prediction confidence of $\mathbf{P^{3D}}$ is sampled from $\mathbf{H^{3D}}$ in the same manner. Second, we concatenate the sampled image features, $\mathbf{P^{3D}}$ , and the prediction confidence of $\mathbf{P^{3D}}$ , to attain $\mathbf{F}^M\in \Reals^{J_c \times (C^\prime+3+1)}$ . Last, we process $\mathbf{F}^M$ using a graph convolutional network (GCN), and predict $\theta^g$ , VPoser [39] latent code $z$ , $\beta$ , and $k$ from output features of GCN with separate MLP layers. $\theta$ is decoded from $z$ . The GCN shows faster convergence during training than an MLP network, and we think the reason lies behind the character of $\theta$ . $\theta$  is parent joint-relative joint rotations, and the GCN can exploit the human kinematic prior different from an MLP. For example, the GCN can implicitly learn the valid range of each parent joint-relative joint leveraging the relationship between human joints.

接下来，基于关节的回归器估计人的全局旋转 $\theta^g \in \Reals^3$ ，SMPL车身旋转参数 $\theta \in \Reals^{21\times3}$ ，SMPL形状参数 $\beta \in \Reals^{10}$ 和相机参数 $k\in \Reals^3$ 用于投影。首先，使用 $\mathbf{P^{3D}}$ 的 $(x,y)$ 像素位置从 $\mathbf{F^\prime}$ 采样每个关节的图像特征。我们使用双线性插值，因为 $(x,y)$ 像素位置不在离散值中。以相同的方式从 $\mathbf{H^{3D}}$ 采样 $\mathbf{P^{3D}}$ 的预测置信度。其次，我们将采样的图像特征 $\mathbf{P^{3D}}$ 和 $\mathbf{P^{3D}}$ 的预测置信度连接起来，以获得 $\mathbf{F}^M\in \Reals^{J_c \times (C^\prime+3+1)}$ 。最后，我们使用图卷积网络（GCN）对 $\mathbf{F}^M$ 进行处理，并通过具有独立MLP层的GCN的输出特征预测 $\theta^g$ 、VPoser[39]潜码 $z$ 、 $\beta$ 和 $k$ 。$\theta$ 是从 $z$ 解码的。GCN在训练过程中表现出比MLP网络更快的收敛性，我们认为原因在于 $\theta$ 的特性。 $\theta$ 是父关节相对关节旋转，GCN可以利用不同于MLP的人类运动学先验。例如，GCN可以利用人类关节之间的关系隐式学习每个父关节相对关节的有效范围。





