
# 1 局部平均区域（Local Averaging Region）

![image-20220619080754497](https://qglh-tuchuang.oss-cn-hangzhou.aliyuncs.com/markdown_img/202206190807562.png)



# 2 法向量（Normal Vectors）

$\quad$ 对于每个三角面的法向量是唯一确定的，但是对于顶点法线可以定义为局部单环邻域中法向量的空间平均。计算公式如下：
$$
\mathbf{n}(v) = \frac{\sum_{T\in\Omega(v)} \alpha_T \ \mathbf{n}(T)}
{\|\sum_{T\in\Omega(v)}\alpha_T \ \mathbf{n}(T)\|}_2 \tag{1}
$$

1. 均值权：$\alpha_T = 1$
2. 三角形面积：$\alpha_T = area(T)$
3. 入射三角角度：$\alpha_T = \theta(T)$



# 3 重心坐标（Barycentric coordinate）

![image-20220619081520746](https://qglh-tuchuang.oss-cn-hangzhou.aliyuncs.com/markdown_img/202206190815772.png)

$\quad$ 对于重心坐标 $g$ ，其计算公式如下：
$$
g = \alpha g_i + \beta g_j + \gamma g_k \tag{2}
$$
$\quad$ 其中：

1. $\alpha + \beta + \gamma = 1$
2. $\alpha,\beta,\gamma \geq 0$
3. $\alpha = \dfrac{s_i}{s_i+s_j+s_k}$ ，其中 $s_i$ 是图中绿色区域，对于 $\beta,\gamma$ 计算公式可以类比。



# 4 梯度（Gradients）

![image-20220619165117349](https://qglh-tuchuang.oss-cn-hangzhou.aliyuncs.com/markdown_img/202206191651384.png)

$\quad$ 有分段线性函数 $f(\mathbf{x}) = \alpha f_i + \beta f_j + \gamma f_k$ 。梯度等于两端对 $\mathbf{x}$ 求偏导，即 

$$
\triangledown _xf(x)= f_i \triangledown _x \alpha + f_j \triangledown _x \beta + f_k \triangledown_x \gamma \tag{3}
$$

## 4.1 公式推导

$\quad$ 首先求 $\alpha$ ：
$$
\alpha = \frac{A_i}{A_T} = \frac
{\Bigg((x-x_j)\cdot \dfrac{(x_k-x_j)^\perp}{\|x_k-x_j\|_2}   \Bigg)\|x_k-x_j\|_2 }
{2A_T} =(x-x_j)\cdot\frac{(x_k-x_j)^\perp}{2A_T} \tag{4}
$$

1.  $(x_k-x_j)^\perp$ 代表 $(x_k - x_j)$  这个向量逆时针旋转 $90\degree$ 。
2. $\|x_k-x_j\|_2$ 代表 $(x_k-x_j)$ 这个向量的长度。
3. 本质上求 $A_i$ 是利用了三角形面积等于底乘高除2的公式。

$\quad$ 紧接着，再对 **公式（4）** 两边对 $x$ 求导，则可求得 $\triangledown_x\alpha$ ，其余同理可得：
$$
\triangledown _x \alpha = \dfrac{(x_k - x_j)^\perp}{2A_T} \\
\triangledown _x \beta = \dfrac{(x_i - x_k)^\perp}{2A_T}\\
\triangledown _x \gamma = \dfrac{(x_j - x_i)^\perp}{2A_T} \tag{5}
$$

$\quad$ 又因为：
$$
(x_k - x_j)^\perp + (x_i - x_k)^\perp + (x_j - x_i)^\perp = 0 \tag{6}
$$
$\quad$ **公式（6）** 相当于把原来环形向量和逆时针旋转 $90\degree$ ，故和为 $0$ ，将 **公式（5）** 和 **公式（6）** 代入 **公式（3）** ，可得：
$$
\triangledown _xf(x) = (f_j-f_i) \cdot \frac{(x_i-x_k)^\perp}{2A_T} + (f_k-f_i) \cdot \frac{(x_j-x_i)^\perp}{2A_T} \tag{7}
$$
$\quad$ 梯度特点：

1. 对每个面上是常值。
2. 不同面上值不同。
3. 顶点上没有定义。



## 4.2 其他

![image-20220619165117349](https://qglh-tuchuang.oss-cn-hangzhou.aliyuncs.com/markdown_img/202206191651384.png)

$\quad$ 怎么求 $(x_k - x_j)^\perp$ 呢？可以叉乘求出面单位法向量 $\mathbf{n} = \dfrac{(x_k-x_j)\times(x_i-x_j)}{\|(x_k-x_j)\times(x_i-x_j)\|_2}$ 。然后 $(x_k - x_j)^\perp = \mathbf{n} \times (x_k-x_j)$ 。



 

# 5 离散拉普拉斯算子

$\quad$ 离散拉普拉斯算子得特点：

+ 天下没有免费的午餐，期望的属性：零性（NULL）、对称性（SYMMETRY）、局部性（LOCALITY）、线性（LINEAR PRECISION）、正权性（POSITIVE WEIGHTS）、半正定性（POSITIVE SEMI-DEFINITENESS）。
  + SYMMETRY + POSITIVE WEIGHTS $\rightarrow$ POSITIVE SEMI-DEFINITENESS
  + POSITIVE SEMI-DEFINITENESS $\nrightarrow$ POSITIVE WEIGHTS  
  + Tutte’s embedding theorem: LOCALITY + LINEAR PRECISION + POSITIVE WEIGHTS.  

+ 三角形曲面网格上的离散拉普拉斯算子跨越了整个几何处理应用领域：网格过滤、参数化、姿势转换、分割、重建、重网格、压缩、模拟和重心坐标插值。
+ 面上的恒定梯度 $\rightarrow$ 面上的拉普拉斯值为零。
+ 存在于顶点上（边上其实也存在）。
+ 基于顶点上的离散拉普拉斯算子：

$$
(Lf)_i  = \sum\limits_{j\in \Omega(i)} \; \omega_{ij}(f_j-f_i) \tag{8}
$$

## 5.1 均值拉普拉斯

$\quad$ 令 $\omega_{ij}=1$ 或 $\omega_{ij} = \frac{1}{N_i}$ ，代入 **公式（8）** 得：
$$
(Lf)_i  = \sum\limits_{j\in \Omega(i)} \; (f_j-f_i)\quad or \quad 
(Lf)_i  = \frac{1}{N_i} \sum\limits_{j\in \Omega(i)} \; (f_j-f_i) \tag{9}
$$
$\quad$ 违反线性（LINEAR PRECISION）；这种定义仅依赖于网格的连接关系，不能反映顶点的空间分布。

## 5.2 余切公式

![image-20220619174353709](https://qglh-tuchuang.oss-cn-hangzhou.aliyuncs.com/markdown_img/202206191743745.png)

$\quad$ 混合了有限元/有限体积的方法，假定在每个顶点上是常值：
$$
\int_{A_i} \triangle f \, dA = \int_{A_i} div\,\triangle f \, dA = \int_{\partial A_i}(\triangledown f) \cdot \mathbf{n} \, ds \tag{10}
$$

1. $A_i$ 是顶点 $i$ 的局部领域定义域。
2. $\partial A_i$ 是 $A_i$ 的边界
3. $\mathbf{n}$ 是边界的向外指向单位法线。
4. $f$ 是在网格上定义的信号。

$\quad$ 首先考虑每个三角形 $T$ 上的积分：
$$
\int_{\partial A_i \cap T} (\triangledown f) \cdot \mathbf{n} \, ds 
=\triangledown f \cdot (\mathbf{a}-\mathbf{b})^\perp 
=\frac{1}{2} \triangledown f \cdot (x_j-x_k) ^ \perp \tag{11}
$$
$\quad$ 其中 $\triangledown f$ 对于每个三角形是常值，如 **公式（7）** 。代入 **公式（11）** 后可得：
$$
\int_{\partial A_i \cap T} (\triangledown f) \cdot \mathbf{n} \, ds  
= (f_j-f_i) \cdot \frac{(x_i-x_k)^\perp \cdot (x_j-x_k) ^ \perp }{4A_T} + (f_k-f_i) \cdot \frac{(x_j-x_i)^\perp \cdot (x_j-x_k) ^ \perp}{4A_T} \tag{12}
$$
$\quad$ 因为：
$$
A_T = \frac{1}{2} \sin(\gamma_j) \|x_j-x_i\|\,\|x_j-x_k\| \\
=\frac{1}{2} \sin(\gamma_k) \|x_i-x_k\|\,\|x_j-x_k\| \tag{13}
$$

$$
\cos(\gamma_j) = \frac{(x_j-x_i)\cdot(x_j-x_k)}{\|x_j-x_i\|\|x_j-x_k\|} \\
\cos(\gamma_k) = \frac{(x_i-x_k)\cdot(x_j-x_k)}{\|x_i-x_k\|\|x_j-x_k\|}  \tag{14}
$$

$$
(x_i-x_k)^\perp \cdot (x_j-x_k)^\perp = (x_i-x_k) \cdot (x_j-x_k) \\
(x_j-x_i)^\perp \cdot (x_j-x_k)^\perp = (x_j-x_i) \cdot (x_j-x_k) \tag{15}
$$

$\quad$ 因此：
$$
\int_{\partial A_i \cap T} \triangledown f \cdot \mathbf{n} \, ds = \frac{1}{2}
\Big(\cot(\gamma_k)\;(f_j-f_i) + \cot(\gamma_j)\;(f_k-f_i)  \Big) \tag{16}
$$
$\quad$ 将 **公式（16）** 代入 **公式（10）** 可得：
$$
\int_{A_i} \triangle f\, dA = \frac{1}{2}\sum\limits_{j \in \Omega(i)} \Big(\cot(\alpha_{ij}) + \cot(\beta_{ij})\Big)(f_j-f_i) \tag{17}
$$
$\quad$ 根据 **公式（17）** 可得在顶点 $v_i$ 处的离散拉普拉斯算子为：
$$
\triangle f(v_i) = \frac{1}{2A_i} \sum\limits_{j\in\Omega(i)}\Big(\cot(\alpha_{ij}) + \cot(\beta_{ij})\Big)(f_j-f_i) \tag{18}
$$

+ 这是最广泛使用的离散化
+ 如果 $\alpha_{ij}+\beta_{ij} > \pi$ ，则 $\cot(\alpha_{ij}) + \cot(\beta_{ij}) < 0$ ，这违背了正权性（POSITIVE WEIGHTS），可能会造成一些不好的后果。
+ 没有免费午餐：并非所有网格都同时满足特性（对称性）、（局部性）、（线性性）和（正权性）。



# 6 离散曲率

![image-20220619181031566](https://qglh-tuchuang.oss-cn-hangzhou.aliyuncs.com/markdown_img/202206191810608.png)

$\quad$ 对于 **公式（18）** ，当 $f$ 等于顶点坐标 $x$ 时，拉普拉斯算子就等于平均曲率法向的离散近似：$\triangle x=-2H\mathbf{n}$ 。两边取绝对值即可得到平均曲率大小公式 $H_i = \dfrac{1}{2}\|\triangle x\|$ 。而离散高斯曲率公式为 $K_i = \dfrac{1}{A_i}\Big(2\pi-\sum\limits_{j\in \Omega(i)} \theta_j\Big)$ 。



